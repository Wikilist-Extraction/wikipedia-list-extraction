package textEvidence

import chalk.text.tokenize.SimpleEnglishTokenizer.V1

import scala.concurrent.Future
import chalk.text.tokenize.SimpleEnglishTokenizer
import chalk.text.analyze.PorterStemmer

object TextEvidenceExtractor {
  val titleWeight = 2/3
  val abstractWeight = 1/3

  val tokenizer = SimpleEnglishTokenizer.V1()

  val abstractQueryString =
    """
      select ?abstract from <http://dbpedia.org> where
      { <$uri> dbpedia-owl:abstract ?abstract. FILTER (langMatches(lang(?abstract),'en')) }
    """

  val titleQueryString =
    """
      select ?title from <http://dbpedia.org> where
      { <$uri> rdfs:label ?title. FILTER (langMatches(lang(?title),'en')) }
    """
}

class TextEvidenceExtractor {
  import TextEvidenceExtractor._

  // TODO: Add to query wrapper
  def executeQuery(uri: String) : String = ???
  // TODO: valid label?, slice prefix, to lowercase
  def cleanType(typeLabel: String) : String = ???

  def normalizeTypes(types: List[String]): List[(String, Iterable[String])] = {
    types map { typeLabel: String =>
      // TODO: Check if label is valid and slice prefixes, to lowercase
      val cleanedLabel = cleanType(typeLabel)

      typeLabel -> tokenizer(cleanedLabel)
    }
  }

  def tokenizeAndStemm(text: String) : Iterable[String] = {
    // TODO: To lowercase
    val tokenized = tokenizer(text)
    tokenized.map(token => PorterStemmer(token))
  }

  def getAbstractsAndTitle(uris: List[String]) : List[(Iterable[String], Iterable[String])] = {
    val abstractsMap = for (uri <- uris) yield {
      // TODO: Add to FragmentsWrapper
      val titleText = executeQuery(titleQueryString.format(uri))
      val abstractText = executeQuery(abstractQueryString.format(uri))

      (tokenizeAndStemm(titleText), tokenizeAndStemm(abstractText))
    }

    abstractsMap
  }

  def countType(typeLabels: Iterable[String], text: Iterable[String]) : Int = {
    val counts = typeLabels map { typeLabel: String =>
      var count = 0
      for (token <- text) {
        if (typeLabel == token) { count += 1 }
      }

      typeLabel -> count
    }

    // return min count
    counts.min._2
  }

  def rateTypes(types: List[(String, Iterable[String])], titles: Iterable[String], abstracts: Iterable[String]) : List[(String, Int)] = {
    val typeCounts = types map { typeTupel =>
      val typeLabel = typeTupel._1
      val normalizedTypes = typeTupel._2
      var count = 0
      count += countType(normalizedTypes, titles) * titleWeight
      count += countType(normalizedTypes, abstracts) * abstractWeight

      typeLabel -> count
    }

    typeCounts
  }

  def rateTypes(types: List[(String, Iterable[String])], abstractsMap: List[(Iterable[String], Iterable[String])]) : List[(String, Double)] = {
    val typeCounts = types map { typeTupel =>
      val typeLabel = typeTupel._1
      val normalizedTypes = typeTupel._2

      var titleCount: Double = 0
      var abstractCount: Double = 0

      for (abstractTupel <- abstractsMap) {
        val abstractTitle = abstractTupel._1
        val abstractText = abstractTupel._2

        titleCount += countType(normalizedTypes, abstractTitle)
        abstractCount += countType(normalizedTypes, abstractText)
      }

      typeLabel -> (titleCount * titleWeight + abstractCount * abstractWeight)
    }

    typeCounts
  }

  /* Expects a list of uris for each list entity and a list of all types (generated by tf-idf) */
  def compute(uris: List[String], types: List[String]) : List[(String, Double)] = {
    val normalizedTypes = normalizeTypes(types)
    val abstractsMap = getAbstractsAndTitle(uris)

    rateTypes(normalizedTypes, abstractsMap)
  }

}
